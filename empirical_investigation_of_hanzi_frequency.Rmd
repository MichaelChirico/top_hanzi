---
title: "An Empiricist's Guide to Learning Chinese Characters"
author: "Michael Chirico"
date: "`r format(as.POSIXlt(Sys.time()), usetz = TRUE)`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.path = 'figures/')
library(data.table)
library(magrittr)
library(sp)
library(viridis)

topN = 500L
n_colors = 50L
char_row = 25L

plot_grid = SpatialPolygonsDataFrame(
  as(GridTopology(c(.5, .5), c(1, 1), 
                  c(char_row, topN/char_row)),
     'SpatialPolygons'),
  # default plot order goes by row,
  #   to order going down columns use:
  #   ID = c(matrix(1:500, 25, 20, byrow = TRUE))
  data = data.frame(ID = seq_len(topN)), match.ID = FALSE
)
plot_xy = coordinates(plot_grid)

colors = viridis(n_colors)
# should text over the colors be black or white?
#   based on this answer here?
# https://stackoverflow.com/a/3943023/3576984
black_white = function(r, g, b) {
  is_black = 0.2126 * r + 0.7152 * g + 0.0722 * b > 0.179
  c('#ffffff', '#000000')[is_black + 1L]
}
color_bw = apply(colorspace::hex2RGB(colors, gamma = TRUE)@coords, 1L,
                 function(rgb) black_white(rgb['R'], rgb['G'], rgb['B']))
```

Having recently moved to Singapore (where _linguae francae_ Mandarin and English are joined by Tamil and Malay as official languages, [see this day-to-day staple](https://c1.staticflickr.com/6/5555/15229811561_576a7f4c04_b.jpg)) and joined a team with a large presence of native Mandarin speakers, I decided to get my act in gear (again) and dive into learning Mandarin.

<!-- add cultural caveats above -- using simplified characters, arbitrarily adding spaces as desired, being pedantic about including tones, being pedantic about Mandarin vs. Chinese, etc -->

<!-- convert some of the below to footnotes/generally condense -->

<!-- footnote re: encoding nightmares? -->

<!-- interactive/clickable plot with links to dictionary -->

<!-- note re: biang? https://en.wikipedia.org/wiki/Biangbiang_noodles -->

 <!-- footnote re: japanese advantage? -->
 
I think there are four major hurdles to learning Mandarin, at least for a native English speaker such as myself:

 1. It's tonal -- how you say pretty much _every_ syllable matters (just try mispronouncing the innocuous "Do you want water?"~"你想水吗?" and watch the grins of the natives). English has hints of this -- my canonical examples are the words "what" and "hmm" (also profanity). It's hard to convey in text, but here goes. There's the curt "what." of impatience which maps to Mandarin's "4th"/"down" tone, "whàt"; the incredulous "whaaat" maps to the "1st"/"flat" tone, "whāt"; the basic inquisitive "what?" rises and corresponds to the "2nd"/"up" tone, "whát"; and finally the facetious "Whro, moi?"-style "what" that basically matches Mandarin's "3rd"/"bouncing" tone "whǎt" (which is a pain to type). The point is, _how_ someone says "what" impacts your parsing of it. If you think this is confusing, imagine being able to distinguish the six tones of Vietnamese, the five tones of Thai, or the _nine_ tones of Cantonese. It's like transforming into a [mantis](https://www.youtube.com/watch?v=F5FEj9U-CJM) [shrimp](http://theoatmeal.com/comics/mantis_shrimp) and gaining new photoreceptors. 
 
 2. There are completely novel vowels and consonants. Tones aside, pronouncing xu and shu as distinct (and moreover being able to _hear_ difference) is nigh impossible for beginners who are not speech therapists or phoneticists and have never heard of [palatal consonants](https://en.wikipedia.org/wiki/Alveolo-palatal_consonant). The same goes for xiang/shang, zhu/ju, etc. Throw into the mix the almost-whistled u vowel (e.g., in the word for fish, 鱼=yú) and the umlauted u (ü) which is found in the word for woman (女=nǚ) and your oral confidence is bound to be in the gutter for quite some time while you try and wrap your tongue around these new shapes (it probably took me the better part of an on-and-off decade to be able to half-confidently roll my "r"s to speak Spanish, for reference). I just encountered 准确=zhǔnquè and it's giving me nightmares.
 
 3. [成语=chéngyǔ](https://en.wikipedia.org/wiki/Chengyu), the four-character idiomatic expressions which are ubiquitous in common parlance (the now-common English expression "long time no see" [may](https://en.wikipedia.org/wiki/Long_time_no_see) derive from the Mandarin 好久不见=hǎojiǔ bùjiàn). This is of course a problem in most languages, but as an American, it feels particularly intimidating for Mandarin, where so few of these idioms are shared, compounded by the almost unfathomably long history of China on and throughout which this vocabulary has been assembled.
 
 4. The pictographic characters (汉字=hànzi) themselves. Knowing the Roman alphabet gets you ready to read (up to a first order approximation) new words in a huge swathe of new languages -- French, Spanish, Portugese, Italian, German, Czech, Swedish, Finnish, Turkish, even as remote as Vietnamese all have their share of quirky accents/symbols, but by and large can be grokked as essentially cognate at first glance, on day one of your studies. Mandarin has pinyin, but you're basically limited to spoken Mandarin without being conversant in characters -- and even then, an ever-present ritual of introductions among Chinese is to establish who has what characters in their name. Your eyes will glaze over quickly if you don't have a mental model of at least the most common characters to fall back on. Even worse, it's nigh on impossible for a complete neophyte to even _use a dictionary_ in Chinese (the question of [how dictionaries in Chinese are sorted](https://www.quora.com/How-are-Chinese-dictionaries-organized) is a fascinating one when considering that this organization was happening thousands of years before the possibility of [Roman] alphabetic sorting arrived to the Orient). Even character recognition keypads on phones typically expect the user to have a basic understanding of stroke order (which, by the way, is well-defined, and all literate natives know by instinct), so simply mimicking drawing the characters is likely to be fruitless in many cases.
 
The remainder of this post will focus on the final point. Depending on your source, there are [as many as **100,000**](https://en.wikipedia.org/wiki/Chinese_characters#Number_of_characters) distinct characters in total. Unless you're some kind of wizard, it's not realistic to try and achieve that level of comprehension, perhaps even for native speakers. The fact is, most native literate Chinese are comfortable with something like 3,000-4,000 characters. Even [Unicode only provides code points](https://stackoverflow.com/a/41155368/3576984) for about 28,000 characters in all of Chinese, Japanese, and Korean, which includes significant duplication owing to often two and sometimes three depictions of the same character depending on the level of [simplification](https://en.wikipedia.org/wiki/Chinese_Character_Simplification_Scheme), e.g. labor (劳 in simplified Chinese, 労 in Japanese, and 勞 in traditional Chinese).

That said, you're a busy person, with finite time to commit to learning this funny new system of expressing yourself and otherstanding others in writing. How should you go about doing so effectively? 

There are several valid answers to this question. This post will dedicate itself to helping you proceed according to one particular answer, namely -- you should study first those characters which appear most frequently. 

## A First Pass at Empirical Frequency: Xinhua

Now that we've defined our goal clearly, it should be _easy_ to provide a quantitative answer -- all we've got to do is find a suitable dataset!

With this in mind, I fired up my [`xpath`](https://www.w3schools.com/xml/xpath_intro.asp) skills and navigated on over to the website of China's official state media organization, [Xinhua=新华](http://www.xinhuanet.com/). Even if you don't understand a white of Chinese, knowing that it's a news website, the basic structure should be clear. The day's top headlines (at least according to Xinhua's editors) are presented with pictures and links to articles.

I decided to pull all the characters from all links (`<a>` nodes) on the top page, then follow all of the internal links on the first page and pull the characters from the linked pages as well. Some links/photo captions/paragraphs are intentionally duplicated, so subsequent instances of identical blocks were eliminated. Full code is available on the [project's GitHub repo](https://github.com/MichaelChirico/top_hanzi); here's the core snippet that extracts and counts the characters from an article:

```{r counts_from_xinhua_article, eval = FALSE}
article_counts = read_html_or_sleep(article) %>% 
  html_nodes(xpath = '//p|//div[@class="h-title"]') %>% 
  blocks_to_zi %>% zi_counts
```

`article` is a URL string; [`blocks_to_zi`](https://github.com/MichaelChirico/top_hanzi/blob/master/utils.R#L12-L15) takes nodes, converts them to text, strips non-Chinese characters, eliminates duplicates, then converts strings to individual characters; [`zi_counts`](https://github.com/MichaelChirico/top_hanzi/blob/master/utils.R#L19-L22) takes a vector of characters and converts them to a [`data.table`](https://github.com/Rdatatable/data.table) of unique characters and their frequency in the vector.

```{r read_xinhua, echo = FALSE, message = FALSE}
xinhua = fread('data/xinhua_counts.csv')
setorder(xinhua, -N)
# not ranking since don't want to deal with ties
xinhua[ , I := .I]

xinhua_unique_n = prettyNum(nrow(xinhua), big.mark = ',')

xinhua_n = xinhua[ , sum(N)]
xinhua_n_order = floor(log10(xinhua_n))
xinhua_n_approx = sprintf('%%%d.0f', xinhua_n_order) %>%
  sprintf(round(xinhua_n, -xinhua_n_order)) %>%
  prettyNum(big.mark = ',')
```

Doing so created a dataset of roughly `r xinhua_n_approx` appearances of `r xinhua_unique_n` distinct characters. What are the most common?

```{r plot_xinhua, results = 'hide', echo = FALSE, fig.height = 9}
# add the top characters to plot_grid
plot_grid = merge(plot_grid, xinhua, by.x = 'ID', by.y = 'I')

color_idx = findInterval(plot_grid$N, seq(0, max(plot_grid$N), 
                                          length.out = n_colors),
                         rightmost.closed = TRUE)

plot(plot_grid, col = colors[color_idx],
     main = sprintf('%d Most Frequent Characters in Xinhua\n%d Per Row', 
                    topN, char_row))
text_col = color_bw[color_idx]
text(plot_xy, labels = plot_grid$zi, col = text_col)
```

This plot shows the top `r topN` characters on Xinhua. Relative frequency is encoded by the background shading of each cell. That there is so little apparent variation in the shade outside the top row owes to the extreme relative concentration in those top rows -- the top character (`r xinhua[1L, zi]`) represents fully `r xinhua[ , sprintf('%.1f%%', 100*N[1L]/sum(N))]` of _all_ characters and is roughly `r xinhua[ , round(N[1L]/N[10L], 1L)]` times more common than even the 10th most frequent, and `r xinhua[ , round(N[1L]/N[100L], 1L)]` times more common than the 100th. The color gradient is much more gradual on the log scale:

```{r plot_xinhua_log, results = 'hide', echo = FALSE, fig.height = 9}
color_idx = findInterval(log(plot_grid$N), seq(0, log(max(plot_grid$N)), 
                                               length.out = n_colors),
                         rightmost.closed = TRUE)

plot(plot_grid, col = colors[color_idx],
     main = sprintf('%d Most Frequent Characters in Xinhua\n%d Per Row (Log Scale)', 
                    topN, char_row))
text_col = color_bw[color_idx]
text(plot_xy, labels = plot_grid$zi, col = text_col)
```

A bit more on what these characters mean later on; for now, here's how common 的 is anecdotally, from an [article](http://www.xinhuanet.com/politics/2018lh/2018-03/17/c_1122550851.htm) about the recent activity in the Chinese Parliament I just pulled up now:

![的 highlighting](figures/de_screenshot_xinhua.png)

(Many of the other top characters also make an appearance -- see if you can spot them!)

Another way of visualizing the concentration intimated above is to depict the CDF of the distributi-on of characters (ordering the discretization starting from the most common character). This allows us to judge percentiles at a glance:

```{r xinhua_percentiles, results = 'hide', echo = FALSE}
xinhua[ , {
  y = 100*cumsum(N)/sum(N)
  par(las = 1L)
  plot(I, y, type = 'l', lwd = 3L, yaxt = 'n',
       xlab = 'Rank of Character Frequency', 
       ylab = 'Cumulative %', col = 'darkgreen',
       main = 'CDF of Character Frequency in Xinhua')
  half <<- which.min(y < 50)
  segments(par('usr')[1L], y[half], I[half], y[half], lty = 2L, col = 'red')
  segments(I[half], 0, I[half], y[half], lty = 2L, col = 'red')
  segments(par('usr')[1L], y[topN], I[topN], y[topN], lty = 2L, col = 'red')
  segments(I[topN], 0, I[topN], y[topN], lty = 2L, col = 'red')
  
  axis(side = 1L, at = I[c(half, topN)], labels = I[c(half, topN)])
  axis(side = 2L, at = c(0, y[c(half, topN)], 100), 
       labels = c(0, 50, round(y[topN]), 100))
}]
```

The slope of this curve at zero is quite sharp, which is just another way of thinking about the concentration that we observed above. We can also see that, despite an intimidating _total_ count of characters, the number of characters required to get a general gist of an article (defined as being able to understand half of its characters) is only `r half`! This is non-trivial (definitely won't be done in a day, a week, or even a month if you're studying part-time), but the Himalayan mountain of 100,000 characters has been reduced to a manageable Appalachian foothill (or, how about [Bukit Timah Hill](https://en.wikipedia.org/wiki/Bukit_Timah_Hill) for local context, though even that is still an order of magnitude too relatively large). `r xinhua[N == 1L, .N]` characters only appeared once in my sample; fully `r xinhua[ , sprintf('%.0f%%', 100*mean(N <= 5))]` appear at most 5 times.

So, there we go, right? We can now focus our efforts on mastering the characters in this chart and get back to learning grammar, vocabulary, practicing speech, etc.

Except -- not so fast. What this chart tells us is that knowing these 500 characters will enable us to understand `r xinhua[ , sprintf('%.0f%%', sum(N[1:topN])/sum(N))]` of characters _in Xinhua articles_, _from the day on which I sampled these articles_ back in November 2017. 

Why is this important? Chinese speakers have probably already picked up on something funny about the top characters; here is the linear-scale plot again with a few of the most salient mis-placed characters highlighted in red:

```{r plot_xinhua_highlight_oddity, results = 'hide', echo = FALSE, fig.height = 9}
color_idx = findInterval(plot_grid$N, seq(0, max(plot_grid$N), 
                                          length.out = n_colors),
                         rightmost.closed = TRUE)

plot(plot_grid, col = colors[color_idx],
     main = sprintf('%d Most Frequent Characters in Xinhua\n%d Per Row', 
                    topN, char_row))
text_col = color_bw[color_idx]
text_col[plot_grid$zi %in% c('中', '国', '新', '华', '网')] = 'red'
text(plot_xy, labels = plot_grid$zi, col = text_col)
```

The third- and fourth- most common characters in this mini-corpus are 中 (zhōng=middle/central) and 国 (gúo=country/kingdom). 中国 means China; it should come as little surprise that a Chinese news organization mentions China at such a high rate. Similarly, 新 (xīn=new) and 华 (huá=flower/China) together are 新华, i.e., the name of the publication (these aren't in sequence in the frequency chart because 新 is independently a fairly common character). 网 (wǎng=net/web) is used in combinations indicating online activity like 上网 (shàngwǎng=sign on) and 网上 (wǎngshàng=online).

In short, the articles we scraped are a biased sample of Chinese characters and we can't be confident of the external validity of what we've found. If your primary purpose in learning characters is to grok the news -- e.g., if you're trying to build a tool for sentiment analysis of Chinese news but don't have time to learn Mandarin or easy access to a native speaker -- this may well be enough. Otherwise, sure, we'll be able to parse Xinhua articles, but will that help us read a menu? Navigate Beijing's subway? Text with friends?

The bias is likely to extend beyond a few oft-repeated words/phrases, too -- journalistic English is quite distinct from other modes of written English (let alone _spoken_ English); we should expect much the same from journalistic Mandarin. Moreover, the dataset described above was a point-in-time snapshot. If you did an analogous exercise on an American media outlet's website right now, you might conclude that "Russia" is one of the most common words in the English language -- news cycle effects are likely to be prominent in such a limited pull of articles.

Overcoming this bias has a tautological solution -- learn Mandarin! But that's rather unsatisfying. What options do I have as a non-speaker to check the robustness of the results I received from Xinhua?

## Let's Try This Again: Zaobao

```{r read_zaobao, echo = FALSE, message = FALSE}
zaobao = fread('data/zaobao_counts.csv')
setorder(zaobao, -N)
zaobao[ , I := .I]

zaobao_unique_n = prettyNum(nrow(zaobao), big.mark = ',')

zaobao_n = zaobao[ , sum(N)]
zaobao_n_order = floor(log10(zaobao_n))
zaobao_n_approx = sprintf('%%%d.0f', zaobao_n_order) %>%
  sprintf(round(zaobao_n, -zaobao_n_order)) %>%
  prettyNum(big.mark = ',')
```


My next idea, given that I'm living in Singapore, was to simply abandon using a Chinese news service in favor of using the Chinese-language paper in the Straits here -- [Zaobao=早报](http://www.zaobao.com.sg/). This required very little in the way of code adjustment, and the basic idea was the same -- find internal links from the top-level page, extract their characters, then extract the characters from the linked articles. This time, I got `r zaobao_n_approx` occurrences of `r zaobao_unique_n` distinct characters. 

```{r plot_zaobao, results = 'hide', echo = FALSE, fig.height = 9}
# wipe earlier data
plot_grid@data = plot_grid@data[ , "ID", drop = FALSE]
plot_grid = merge(plot_grid, zaobao, by.x = 'ID', by.y = 'I')

color_idx = findInterval(plot_grid$N, seq(0, max(plot_grid$N), 
                                          length.out = n_colors),
                         rightmost.closed = TRUE)

plot(plot_grid, col = colors[color_idx],
     main = sprintf('%d Most Frequent Characters in Zaobao\n%d Per Row', 
                    topN, char_row))
text_col = color_bw[color_idx]
text_col[plot_grid$zi %in% c('中', '国', '新', '华', '网')] = 'red'
text_col[plot_grid$zi %in% c('加', '坡', '马', '西', '亚', '洲')] = 'orange'
text(plot_xy, labels = plot_grid$zi, col = text_col)
```

And here's the CDF plot:

```{r zaobao_percentiles, results = 'hide', echo = FALSE}
zaobao[ , {
  y = 100*cumsum(N)/sum(N)
  par(las = 1L)
  plot(I, y, type = 'l', lwd = 3L, yaxt = 'n',
       xlab = 'Rank of Character Frequency', 
       ylab = 'Cumulative %', col = 'darkgreen',
       main = 'CDF of Character Frequency in Zaobao')
  half <<- which.min(y < 50)
  segments(par('usr')[1L], y[half], I[half], y[half], lty = 2L, col = 'red')
  segments(I[half], 0, I[half], y[half], lty = 2L, col = 'red')
  segments(par('usr')[1L], y[topN], I[topN], y[topN], lty = 2L, col = 'red')
  segments(I[topN], 0, I[topN], y[topN], lty = 2L, col = 'red')
  
  axis(side = 1L, at = I[c(half, topN)], labels = I[c(half, topN)])
  axis(side = 2L, at = c(0, y[c(half, topN)], 100), 
       labels = c(0, 50, round(y[topN]), 100))
}]
```

We can immediately see that some of the biases observed in the Xinhua data have indeed been mitigated -- only 国 remains in the same position, likely owing to the fact that 国 appears in many countries' names (美国=USA, 泰国=Thailand, 韩国=Korea) and news in Singapore tends by necessity to be relatively international. Some new idiosyncrasies have arisen in their place, however -- I spot, e.g., prominence for characters in 新加坡=Xīnjiāpó, Singapore's phoneticized Mandarin name; 马来西亚=Mǎláixīyà, the phoneticized name of Malaysia, Singapore's next door neighbor; and 亚洲=Yàzhōu, Asia.

Despite this, the shape of the CDF is remarkably similar, as the number of characters (`r half`) required to understand half of those found in Zaobao articles is quite similar to that found in Xinhua, as is the total coverage of the top `r topN`. 

So, we can call it a day, right? The percentiles appear to be pretty robust to choice of newspaper, so all this worry about bias was for nought. Or so it's tempting to think. While we've certainly diminished some of the Sino-centrism of Xinhua, we still haven't addressed the deeper/subtler shortcomings of the Xinhua dataset -- it's written as news, about salient news topics at a fixed point in time (close in time, in fact, to the pull of Xinhua data). If all we've done is identify _the_ distribution of characters in journalist's Mandarin, that's pretty unsatisfying.

Personally, I want to learn Mandarin for a much more practical purpose -- my primary target is becoming conversational, which includes being able to communicate in writing informally. With these two data sets, I'm much more likely to unwittingly become a bookish speaker and writer.

## Third Time's a Charm? Wikipedia

Nevertheless, I found myself struggling a bit to find another online corpus ready for analysis, especially one that fit my needs as a language learner. Instead, I identified another corpus of written Chinese data that I thought would be sufficiently orthogonal to that I uncovered in Zaobao and Xinhua that I thought it worth pursuing -- [Chinese Wikipedia](https://zh.wikipedia.org/). 

The biggest appeal of Wikipedia was the potential size -- as of this writing, there are [roughly one million articles](https://en.wikipedia.org/wiki/List_of_Wikipedias) on the Mandarin version of Wikipedia.

